{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29d1610",
   "metadata": {},
   "source": [
    "# This code file is to prepare for the data that we are going to use for the analysis\n",
    "\n",
    "## What Files Are Given to Start with:\n",
    "\n",
    "**us_cities_by_state_SEPT.2023.csv**: the names/urls of the articles we would be working with. This data is scraped from [this WikiPedia page](https://en.wikipedia.org/wiki/Category:Lists_of_cities_in_the_United_States_by_state) and the scraped result could be found [here](https://drive.google.com/file/d/1khouDmMaZyKo0y5WkFj4lu7g8o35x_98/view?usp=sharing)\n",
    "\n",
    "**NST-EST2022-ALLDATA.csv**: this data is obtained from [here](https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html). I chose the full dataset instead of the excel file because the excel file is not in a table. I took out only the names of states and the estimated population of the states for the analysis\n",
    "\n",
    "**US States by Region in Table - US Census Bureau.csv**: the original dataset *US States by Region - US Census Bureau.csv* comes from [this file](https://docs.google.com/spreadsheets/d/14Sjfd_u_7N9SSyQ7bmxfebF_2XpR8QamvmNntKDIQB0/edit?usp=sharing)\n",
    "\n",
    "## What Tools Used in this Project:\n",
    "\n",
    "**Wikimedia**:  to extract the page information of articles. [Sample code used in this project comes from here](https://drive.google.com/file/d/15UoE16s-IccCTOXREjU3xDIz07tlpyrl/view?usp=sharing)\n",
    "\n",
    "**ORES model**: provides the peer reviewed score of each article based on the quality. [Sample code used in this project comes from here](https://drive.google.com/file/d/17C9xsmR9U3lJeD52UTbAedlHDetwYsxs/view?usp=sharing)\n",
    "\n",
    "## What Steps Are Covered in this Code File:\n",
    "\n",
    "**Scraping the page information of articles** - using the *Wikimedia API*\n",
    "\n",
    "**Obtaining the assessed quality score for each article** - using the ORES model. A second attempt was performed after noting down which articles failed to retrieve its score, and only one failed again to retrieve its score on the second try which was removed. It is also worth noticing that there exist duplicates of rows of data which should be removed in order to perform the analysis\n",
    "\n",
    "**Combining the datasets** - the previous steps result in a DataFrame with columns of article name, state, review id and score. Population (which we could obtain from the *NST-EST2022-ALLDATA.csv*) and regional data (which is in *US States by Region in Table - US Census Bureau.csv*) are required in the final dataset, which should contain in total 6 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f664d7f6",
   "metadata": {},
   "source": [
    "### Scraping using Wikimedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8cd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl --upgrade\n",
    "import pandas as pd\n",
    "import json, time, urllib.parse\n",
    "import requests\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "pop_est_2022 = pd.read_csv('NST-EST2022-ALLDATA.csv')\n",
    "cities_by_state = pd.read_csv('us_cities_by_state_SEPT.2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9bbfc",
   "metadata": {},
   "source": [
    "### Obtained the list of the titles of the articles whose page information we are looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938241c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles = list(cities_by_state['page_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': 'hww1999@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "\n",
    "ARTICLE_TITLES = article_titles\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ca6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09860dfb",
   "metadata": {},
   "source": [
    "### Handled exceptions here to prevent failure to retrieve\n",
    "\n",
    "One intermediary data file generated here for backup purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efe55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_by_state_articles = {\"articles\":[]}\n",
    "for i in range(len(ARTICLE_TITLES)):\n",
    "    print(\"Getting page information for: \",ARTICLE_TITLES[i])\n",
    "    views = request_pageinfo_per_article(ARTICLE_TITLES[i])\n",
    "    try:\n",
    "        pid = list(views['query']['pages'].keys())[0]\n",
    "        cities_by_state_articles[\"articles\"].append(views['query']['pages'][pid])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "with open(\"cities_by_state.json\", \"a\") as outfile:\n",
    "        outfile.write(json.dumps(cities_by_state_articles,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_json('cities_by_state.json')['articles']\n",
    "tmp = pd.json_normalize(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f80581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = tmp[['pageid', 'title', 'lastrevid', 'talkid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba799d0",
   "metadata": {},
   "source": [
    "### Evaluation using ORES model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e54fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<{email_address}>, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "#\n",
    "ARTICLE_REVISIONS = {'Abbeville, Alabama': 1171163550}\n",
    "# { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe606cc1",
   "metadata": {},
   "source": [
    "### Personal access token is saved in a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input personal information\n",
    "# access token saved as a local file\n",
    "\n",
    "token_info = pd.read_csv('WikiAPIToken.csv')\n",
    "ACCESS_TOKEN = token_info['Access token'][0]\n",
    "USERNAME = \"\" # email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb971981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b570673b",
   "metadata": {},
   "source": [
    "### Below is the first attempt with 107 failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb91584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_pred = []\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(df_eval.shape[0])):\n",
    "    \n",
    "    article_title = df_eval.iloc[i]['title']\n",
    "    article_revid = df_eval.iloc[i]['lastrevid']\n",
    "    \n",
    "#     print(f\"Getting LiftWing ORES scores for '{article_title}' with revid: {article_revid:d}\")\n",
    "    try:\n",
    "    \n",
    "        score = request_ores_score_per_article(article_revid=int(article_revid),\n",
    "                                               email_address=USERNAME,\n",
    "                                               access_token=ACCESS_TOKEN)\n",
    "\n",
    "        pred = score['enwiki']['scores'][str(article_revid)]['articlequality']['score']['prediction']\n",
    "#         print(article_title + ': ' + pred)\n",
    "        score_pred.append(pred)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(article_title + ': Unknown')\n",
    "        score_pred.append('REQUEST FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c16f2",
   "metadata": {},
   "source": [
    "### The second intermediary dataset generated for back up purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['score_pred'] = pd.Series(score_pred)\n",
    "df_eval.to_csv('score_first_attemp.csv', index=False)\n",
    "df_eval = pd.read_csv('score_first_attemp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e338bac",
   "metadata": {},
   "source": [
    "### At this time, I realized I forgot to include the state column from the given file, so added in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['state'] = cities_by_state['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d451e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failed_first_attempt = df_eval[df_eval['score_pred']=='REQUEST FAIL'][['title', 'lastrevid']]\n",
    "df_failed_first_attempt = df_failed_first_attempt.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ace4c01",
   "metadata": {},
   "source": [
    "### This attempt fixed 106 failed retrievals among 107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75fef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_attempt = []\n",
    "for i in range(df_failed_first_attempt.shape[0]):\n",
    "    \n",
    "    article_title = df_failed_first_attempt.iloc[i]['title']\n",
    "    article_revid = df_failed_first_attempt.iloc[i]['lastrevid']\n",
    "    \n",
    "#     print(f\"Getting LiftWing ORES scores for '{article_title}' with revid: {article_revid:d}\")\n",
    "    try:\n",
    "    \n",
    "        score = request_ores_score_per_article(article_revid=int(article_revid),\n",
    "                                               email_address=USERNAME,\n",
    "                                               access_token=ACCESS_TOKEN)\n",
    "#         print(score)\n",
    "        second_attempt.append(score['enwiki']['scores'][str(article_revid)]['articlequality']['score']['prediction'])\n",
    "#         pred = score['enwiki']['scores'][str(article_revid)]['articlequality']['score']['prediction']\n",
    "#         print(article_title + ': ' + pred)\n",
    "#         second_attempt.append(pred)\n",
    "    except Exception as e:\n",
    "#         print(e)\n",
    "#         print(article_title + ': Unknown')\n",
    "        second_attempt.append('REQUEST FAIL')\n",
    "df_failed_first_attempt['score'] = pd.Series(second_attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_failed_first_attempt.reset_index()\n",
    "# df_failed_first_attempt['score'] = pd.Series(second_attempt)\n",
    "df_failed_first_attempt[df_failed_first_attempt['score']=='REQUEST FAIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_failed_first_attempt.shape[0]):\n",
    "    currtitle = df_failed_first_attempt.iloc[i]['title']\n",
    "    currrevid = df_failed_first_attempt.iloc[i]['lastrevid']\n",
    "    currscore = df_failed_first_attempt.iloc[i]['score']\n",
    "    idx = df_eval[df_eval['title']==currtitle][df_eval['lastrevid']==currrevid].index\n",
    "    df_eval.loc[idx,['score_pred']] = currscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee23d7",
   "metadata": {},
   "source": [
    "### Dropped duplicates at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1295915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df_eval.drop_duplicates()\n",
    "df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df_eval.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8fc7c1",
   "metadata": {},
   "source": [
    "### We found that the same article but with a different review id is there with score already, so removed the one that failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval[df_eval['score_pred']=='REQUEST FAIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfd019",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval[df_eval['title']=='Auburn, Alabama']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c62a2",
   "metadata": {},
   "source": [
    "### The third intermediary data file generated which should overwrite the score_first_attemp.csv given that it only filled in the scores that failed to retrieve at the first try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df_eval.drop([461])\n",
    "df_eval = df_eval.reset_index()\n",
    "df_eval.to_csv('updated_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4101067",
   "metadata": {},
   "source": [
    "### Combining the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfe9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv('updated_score.csv')\n",
    "df_eval = df_eval.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df_eval.drop(columns=['level_0', 'index'])\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f85a31",
   "metadata": {},
   "source": [
    "### Noted there is no Connecticut or Nebraska from the original scraped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22788baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_eval['state'].unique()) # no Connecticut or Nebraska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['state'].unique() # need to replace '_' with space, and 'Georgia_(U.S._state)' with 'Georgia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee018deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['state'] = df_eval['state'].str.replace('Georgia_(U.S._state)','Georgia', regex=False)\n",
    "df_eval['state'] = df_eval['state'].str.replace('_',' ', regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72bf76",
   "metadata": {},
   "source": [
    "### We only need the NAME of the states and the POPESTIMATE2022 for the estimated population in 2022 for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_est_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_2022 = pop_est_2022[['NAME', 'POPESTIMATE2022']]\n",
    "pop_2022 = pop_2022.rename(columns={\"NAME\": \"state\", \"POPESTIMATE2022\": \"population\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29438f94",
   "metadata": {},
   "source": [
    "### Join on the state column to obtain the population field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop = pd.merge(df_eval, pop_2022, on=['state'])\n",
    "df_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd29fc",
   "metadata": {},
   "source": [
    "### Find out the region each state belongs to and join on state column again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca64ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_by_region = pd.read_csv('US States by Region in Table - US Census Bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_by_region = states_by_region.rename(columns={\"REGION\": \"regional_division\", \"STATE\": \"state\"})\n",
    "states_by_region = states_by_region.drop(columns=['DIVISION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54aa693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_pop, states_by_region, on=['state'])\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9199e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.rename(columns={\"title\": \"article_title\", \"lastrevid\": \"revision_id\", \"score_pred\": \"article_quality\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c52d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('wp_scored_city_articles_by_state.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
